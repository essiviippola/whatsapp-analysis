{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"whatsapp-text-generation-markov-chains.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPH+IP92DGfyjkRgOJTU6yt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"36DYu34VS4WZ"},"source":["# Whatsapp text generation using Markov chains with back-off\r\n","\r\n","Source: https://blog.dataiku.com/2016/10/08/machine-learning-markov-chains-generate-clinton-trump-quotes"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YAaZ0ehSSvIr","executionInfo":{"status":"ok","timestamp":1612211917826,"user_tz":-120,"elapsed":22181,"user":{"displayName":"Essi Viippola","photoUrl":"","userId":"15219457985143766510"}},"outputId":"2daace7f-2f3a-42b5-8ab6-7b99cf1786f5"},"source":["# Setting up Google Colab\r\n","\r\n","from google.colab import drive\r\n","drive.mount(\"/content/gdrive\")\r\n","\r\n","%cd gdrive/My Drive/Projektit/whatsapp-analysis/src\r\n","\r\n","! pip install emoji"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n","/content/gdrive/My Drive/Projektit/whatsapp-analysis/src\n","Collecting emoji\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/fa/b3368f41b95a286f8d300e323449ab4e86b85334c2e0b477e94422b8ed0f/emoji-1.2.0-py3-none-any.whl (131kB)\n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133kB 9.1MB/s \n","\u001b[?25hInstalling collected packages: emoji\n","Successfully installed emoji-1.2.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9bUw77JuTQsB","executionInfo":{"status":"ok","timestamp":1612211923868,"user_tz":-120,"elapsed":3204,"user":{"displayName":"Essi Viippola","photoUrl":"","userId":"15219457985143766510"}}},"source":["# Import libraries\r\n","\r\n","import numpy as np\r\n","import random\r\n","import nltk\r\n","from whatsapp_analysis.config import data_path\r\n","from whatsapp_analysis.helper import import_data, preprocess_data"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"NJBOKVglTWKj","executionInfo":{"status":"ok","timestamp":1612211931917,"user_tz":-120,"elapsed":3534,"user":{"displayName":"Essi Viippola","photoUrl":"","userId":"15219457985143766510"}}},"source":["# Read and pre-process data\r\n","\r\n","df = import_data(data_path)\r\n","df = preprocess_data(df)\r\n","messages = df[(df['media_count'] == 0) & (df['word_count'] > 1) & (df['link_count'] == 0)]['message']"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u-9KgKKpUq-e","executionInfo":{"status":"ok","timestamp":1612211994131,"user_tz":-120,"elapsed":1272,"user":{"displayName":"Essi Viippola","photoUrl":"","userId":"15219457985143766510"}},"outputId":"e1f7e1c7-52c6-4561-e171-51ef36b6f17a"},"source":["nltk.download('punkt')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gqlkUcKqTcgN","executionInfo":{"status":"ok","timestamp":1612213278355,"user_tz":-120,"elapsed":16213,"user":{"displayName":"Essi Viippola","photoUrl":"","userId":"15219457985143766510"}},"outputId":"464b4bd2-e4ae-46ba-d06a-291c3683e5f1"},"source":["# Source: https://blog.dataiku.com/2016/10/08/machine-learning-markov-chains-generate-clinton-trump-quotes\r\n","\r\n","class markov(object):\r\n","    def __init__(self, corpus, n_grams, min_length):       \r\n","        \"\"\"\r\n","        corpus = list of string text [\"speech1\", \"speech2\", ..., \"speechn\"]\r\n","        n_grams = max sequence length\r\n","        min_length = minimum number of next words required for back-off scheme       \r\n","        \"\"\"\r\n","        self.grams = {}\r\n","        self.n_grams = n_grams\r\n","        self.corpus = corpus   \r\n","        self.min_length = min_length\r\n","        self.sequences()\r\n","\r\n","    def tokenizer(self, speech, gram):\r\n","        \"\"\"tokenize speeches in corpus, i.e. split speeches into words\"\"\"\r\n","        tokenized_speech = nltk.word_tokenize(speech)\r\n","\r\n","        if len(tokenized_speech) < gram:\r\n","            pass       \r\n","        else:\r\n","            for i in range(len(tokenized_speech) - gram):\r\n","                yield (tokenized_speech[i:i + (gram + 1)])\r\n","\r\n","    def sequences(self):\r\n","        \"\"\"\r\n","        create all sequences of length up to n_grams\r\n","        along with the pool of next words.\r\n","        \"\"\"        \r\n","        for gram in range(1, self.n_grams + 1):\r\n","            dictionary = {}            \r\n","            for speech in self.corpus:               \r\n","                for sequence in self.tokenizer(speech, gram):\r\n","                    key_id = tuple(sequence[0:-1])\r\n","\r\n","                    if key_id in dictionary:\r\n","                        dictionary[key_id].append(sequence[gram])\r\n","                    else:\r\n","                        dictionary[key_id] = [sequence[gram]]                        \r\n","            self.grams[gram] = dictionary\r\n","\r\n","    def next_word(self, key_id):\r\n","        \"\"\"returns the next word for an input sequence\r\n","        but backs off to shorter sequence if length\r\n","        requirement is not met.         \r\n","        \"\"\"\r\n","        for i in range(len(key_id)):\r\n","            try:\r\n","                if len(self.grams[len(key_id)][key_id]) >= self.min_length:\r\n","                    return random.choice(self.grams[len(key_id)][key_id])\r\n","            except KeyError:\r\n","                pass\r\n","        # if the length requirement isn't met, we shrink the key_id\r\n","            if len(key_id) > 1:\r\n","                key_id = key_id[1:]\r\n","        # when we're down to only a one-word sequence,\r\n","        #ignore the requirement\r\n","        try:\r\n","            return random.choice(self.grams[len(key_id)][key_id])\r\n","        except KeyError:\r\n","            # key does not exist: should only happen when user enters\r\n","            # a sequence whose last word was not in the corpus\r\n","            # choose next word at random\r\n","            return random.choice(' '.join(self.corpus).split())\r\n","\r\n","    def next_key(self, key_id, res):\r\n","        return tuple(key_id[1:]) + tuple([res])\r\n","\r\n","    def generate_markov_text(self, start, size=25):\r\n","        \"\"\"\"start is a sentence of at least n_grams words\"\"\"\r\n","        key_id = tuple(nltk.word_tokenize(start))[ - self.n_grams:]\r\n","        gen_words = []\r\n","        i = 0\r\n","        while i <= size:\r\n","            result = self.next_word(key_id)\r\n","            key_id = self.next_key(key_id, result)\r\n","            gen_words.append(result)\r\n","            i += 1\r\n","        print('Seed:', start)       \r\n","        print('Result:', ' '.join(gen_words).replace(' .', '.').replace(' ,', ','))\r\n","\r\n","mark = markov(messages, n_grams=5, min_length=5)\r\n","mark.generate_markov_text('mitÃ¤ tehdÃ¤Ã¤n lauantaina', size=50)"],"execution_count":35,"outputs":[{"output_type":"stream","text":["Seed: mitÃ¤ tehdÃ¤Ã¤n lauantaina\n","Result: kÃ¤ydÃ¤ kyselemÃ¤ssÃ¤ ettÃ¤ opin elÃ¤mÃ¤stÃ¤ kun 2/9 * oblivion music television nÃ¤ky, mut ens viikon perjantaina saadaan ihmiset pelkÃ¤Ã¤ mikrojaðŸ˜ƒ TÃ¤Ã¤ puukoriste on kai virallinen asiointikieli on semmonen autossa ja luot yhteyden ottamisest apreeseensissÃ¤ eikÃ¤ edes teamssii pyytÃ¤ny jÃ¤lkikÃ¤teen usb voi ydinvoimalais ni sitten just nyt luet siellÃ¤ kermaviiliÃ¤mpÃ¤reitÃ¤ ?\n"],"name":"stdout"}]}]}